{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Image Captioning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Select appropriate values for the Python variables below.\n",
    "batch_size = 64          # batch size, change to 64\n",
    "vocab_threshold = 3        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_features = 2048        # number of feature maps, produced by Encoder\n",
    "num_epochs = 14             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "\n",
    "log_train = 'training_log.txt'       # name of files with saved training loss and perplexity\n",
    "\n",
    "# Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN()\n",
    "decoder = DecoderRNN(num_features = num_features, \n",
    "                     embedding_dim = embed_size, \n",
    "                     hidden_dim = hidden_size, \n",
    "                     vocab_size = vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "#params = list(decoder.parameters()) + list(encoder.parameters()) \n",
    "params = list(decoder.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr = 1e-4)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, \n",
    "          encoder, \n",
    "          decoder, \n",
    "          optimizer, \n",
    "          criterion, total_step, num_epochs, data_loader, write_file, save_every = 1):\n",
    "    \"\"\" Train function for a single epoch. \n",
    "    Arguments: \n",
    "    ----------\n",
    "    - epoch - number of current epoch\n",
    "    - encoder - model's Encoder\n",
    "    - decoder - model's Decoder\n",
    "    - optimizer - model's optimizer (Adam in our case)\n",
    "    - criterion - loss function to optimize\n",
    "    - num_epochs - total number of epochs\n",
    "    - data_loader - specified data loader (for training, validation or test)\n",
    "    - write_file - file to write the training logs\n",
    "    \n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    epoch_perplex = 0.0\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        # training mode on\n",
    "        encoder.eval() # no fine-tuning for Encoder\n",
    "        decoder.train()\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "        # target captions, excluding the first word\n",
    "        captions_target = captions[:, 1:].to(device) \n",
    "        # captions for training without the last word\n",
    "        captions_train = captions[:, :-1].to(device)\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs, atten_weights = decoder(captions= captions_train,\n",
    "                                         features = features)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        perplex = np.exp(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_perplex += perplex\n",
    "        \n",
    "        stats = 'Epoch train: [%d/%d], Step train: [%d/%d], Loss train: %.4f, Perplexity train: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), perplex)\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        write_file.write(stats + '\\n')\n",
    "        write_file.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "        \n",
    "    epoch_loss_avg = epoch_loss / total_step\n",
    "    epoch_perp_avg = epoch_perplex / total_step\n",
    "    \n",
    "    print('\\r')\n",
    "    print('Epoch train:', epoch)\n",
    "    print('\\r' + 'Avg. Loss train: %.4f, Avg. Perplexity train: %5.4f' % (epoch_loss_avg, epoch_perp_avg), end=\"\")\n",
    "    print('\\r')\n",
    "    \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the training log file.\n",
    "file_train = open(log_train, 'w')\n",
    "\n",
    "for epoch in range(0, num_epochs+1):   \n",
    "    train(epoch, encoder, decoder, optimizer, criterion, total_step, num_epochs =num_epochs,\n",
    "          data_loader = data_loader,\n",
    "          write_file = file_train, \n",
    "          save_every = 1)\n",
    "    \n",
    "file_train.close()\n",
    "file_val.close()\n",
    "bleu_score_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}